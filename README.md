# ğŸŒ¸ IRIS Classification â€” Linear vs MLP (PyTorch)

**Experimental classification of the IRIS dataset** using **PyTorch**, comparing a **single-layer linear classifier** with a **Multi-Layer Perceptron (MLP)** and analyzing the influence of the hidden layer size **H** on classification quality and stability.

ğŸ“„ **Report (PDF):** [Sprawozdanie.pdf](Sprawozdanie.pdf)   
ğŸ“Š **Outputs:** plots + CSV tables saved automatically after each run (`runs/...`)  

---

## ğŸ¯ Goal
This project answers two questions:  
- How well does a **linear (single-layer) model** classify IRIS compared to an **MLP**?  
- How does the number of hidden neurons **H** affect **accuracy**, **variance across seeds**, and **overfitting**?  

---

## ğŸŒ¿ Dataset (IRIS)  
IRIS contains **150 samples**, each described by **4 features** (sepal/petal length & width) and belonging to **3 classes**:  
*setosa*, *versicolor*, *virginica*.  

Source: UCI Machine Learning Repository   
https://archive.ics.uci.edu/dataset/53/iris  

---

## ğŸ§  Models
### ğŸ”¹ Linear classifier (single-layer)  
`Linear(4 â†’ 3)`  
  
trained with **CrossEntropyLoss** (softmax handled internally).  

### ğŸ”¹ MLP (one hidden layer)  
`Linear(4 â†’ H) â†’ ReLU â†’ Linear(H â†’ 3)`  

Tested hidden sizes:  
  
`H âˆˆ {1, 2, 4, 8, 16, 32, 64}`  

---

## âš™ï¸ Experiment setup
- Train/test split: **80/20**, **stratified**  
- Feature scaling: **StandardScaler** (fit on train, applied to test)  
- Optimizer: **Adam**  
- Loss: **CrossEntropyLoss**  
- Epochs: **250**  
- Batch size: **16**  
- Multiple random seeds to estimate stability (**mean Â± std**, boxplots)  

---

## ğŸ“Š Results (mean Â± std across seeds)

| Model  | H  | Accuracy (mean Â± std) |
|--------|----|------------------------|
| linear | â€”  | 0.9733 Â± 0.0327 |
| MLP    | 1  | 0.9667 Â± 0.0422 |
| MLP    | 2  | **0.9800 Â± 0.0267** |
| MLP    | 4  | 0.9600 Â± 0.0533 |
| MLP    | 8  | 0.9667 Â± 0.0365 |
| MLP    | 16 | 0.9667 Â± 0.0422 |
| MLP    | 32 | 0.9333 Â± 0.0365 |
| MLP    | 64 | 0.9467 Â± 0.0452 |

âœ… Best average result: **MLP with H=2**    
âš ï¸ Larger models (e.g. **H=32**) show signs of worse generalization / overfitting on this small dataset.  

---

## ğŸ“ˆ Visualizations

## ğŸ“Š Visualizations

### Accuracy vs hidden size (H)
![Accuracy vs H](runs/iris_20251228_123408/figs/acc_vs_hidden.png)

### Stability across seeds (boxplot)
![Boxplot accuracy vs H](runs/iris_20251228_123408/figs/boxplot_acc_vs_hidden.png)

### Learning curves (train vs test loss)
![Learning curves](runs/iris_20251228_123408/figs/learning_curves_loss.png)

### Confusion matrices (linear vs best MLP)
![Confusion matrix â€” linear](runs/iris_20251228_123408/figs/cm_linear_seed0.png)
![Confusion matrix â€” best MLP](runs/iris_20251228_123408/figs/cm_best_h2_seed0.png)

### PCA-2D decision regions (intuition / visualization)
![PCA decision regions](runs/iris_20251228_123408/figs/pca_decision_regions_best_h2.png)

---

## ğŸ“‚ Output files
Each run creates a timestamped folder under `runs/` with:  
- **figs/** â€” plots (`.png`)  
- **tables/** â€” results tables (`.csv`) + confusion matrices (`.txt`)  
- **models/** â€” saved weights (`.pt`)  

Example:
```text
.
â”œâ”€ main.py
â”œâ”€ Sprawozdanie.pdf
â”œâ”€ results_summary.csv
â”œâ”€ results_per_seed.csv
â”œâ”€ config.json
â”œâ”€ runs/
â”‚  â””â”€ iris_YYYYMMDD_HHMMSS/
â”‚     â”œâ”€ figs/
â”‚     â”œâ”€ tables/
â”‚     â””â”€ models/
â””â”€ assets/
   â”œâ”€ acc_vs_hidden.png
   â”œâ”€ boxplot_acc_vs_hidden.png
   â”œâ”€ learning_curves_loss.png
   â”œâ”€ cm_compare.png
   â””â”€ pca_decision_regions.png
```

---

## â–¶ï¸ Running the project
Install dependencies:   
`pip install numpy torch matplotlib scikit-learn`  
Run:   
`python main.py`
Outputs will be saved under:   
`runs/iris_<timestamp>/`

---

###ğŸ§‘â€ğŸ’» Author
 
Created by Avuii  
